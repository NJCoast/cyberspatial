# Notes on Google Cloud Platform and Kubernetes

The base configuration for Kubernetes is generated by the [kompose](http://kompose.io/) tool from the default docker-compose.yml file (which is the deployment, not development configuration).

Secrets for deployment should be stored in a seperate yml that is NOT commited to github.

A good description of Google Cloud, Kubernetes and Django deployment can be found in this google cloudb [blog](https://medium.com/google-cloud/deploying-django-postgres-redis-containers-to-kubernetes-9ee28e7a146) and [part 2](https://medium.com/google-cloud/deploying-django-postgres-and-redis-containers-to-kubernetes-part-2-b287f7970a33). There is a [github repo](https://github.com/waprin/kubernetes_django_postgres_redis) with all of the code for these blog post examples.

## Setup of Google Cloud using the command line sdk
The google cloud sdk in available in brew casks.
```bash
$ brew cask install google cloud sdk
```

Which will set up the command line tool. The gcloud command line tool needs to be linked to a google cloud project for billing.
```bash
gcloud components update # make sure it’s up to date
gcloud config set project <project-id>
# Create the cluster with 2 nodes
gcloud container clusters create njcoast --scopes "https://www.googleapis.com/auth/userinfo.email","cloud-platform" --num-nodes 2
# Configure kubectl with the right context
gcloud container clusters get-credentials njcoast
```

Which creates a cluster with two nodes. Note that there have been reports of performance issues for less than three nodes.

Here we are creating 2 nodes. Note that for Container Engine, you will be billed for the instances you are creating (but for up to 5 nodes, there are no additional charges).
When you’re done with this tutorial, to avoid recurring charges, delete the instances.

To shut down the cluster instances, and avoid additional charges.
```bash
gcloud container clusters delete njcoast
```

After the cluster is spun up, you should have assess to the Kubernetes deployment using kubectl. The main CLI to interact with the Kubernetes cluster will be kubectl. At first, the only existing Service should be Kubernetes, and you shouldn’t have any pods.
```bash
$ kubectl get services
$ kubectl get pods
```


## Kubernetes on local mac environment
For debugging purposes, it is sometimes easier to test in a local mikikube Kubernetes environment. There is an updated [blog post](https://medium.com/google-cloud/local-django-on-kubernetes-with-minikube-89f5ad100378) that describes how to deploy to straight Kubernetes instead of Google Cloud. To install the environment, I turned off docker for mac to be safe. In principle, docker will try socket file first, the env, so both should be able to coexist.


On MacOS to use the xhyve driver for minikube, docker-machine-driver-xhyve. Homebrew has it in formulas.
```bash
$ brew install docker-machine-driver-xhyve --HEAD
```

Permissions for the driver will need to be set to root to work correctly:
```bash
$ sudo chown root:wheel /usr/local/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve
$ sudo chmod u+s /usr/local/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve
```

Start minicube with docker environment and the xhyve driver (otherwise it trys to grab virtualbox base box instead).
```bash
$ minikube start --vm-driver=xhyve
```
There is a [bug](https://github.com/kubernetes/minikube/issues/1400) with mikikube that seems to be fixed if you change the permissions again after a minikube stop command.
```bash
$ sudo chown root:wheel /usr/local/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve
$ sudo chmod u+s /usr/local/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve
```



You can set this permanently with:
```bash
$ minikube config set vm-driver=xhyve
```

If you are running the docker for mac system, docker will be pointed to this vm by default. To connect to the minikube docker environment to build containers use:

```bash
$ eval $(minikube docker-env)
```

The entire minikube (docker-machine) environment can be printed using:
```bash
$ minikube docker-env
```

Run sample project. Kubectl should work as normal after this using env variables.
```bash
$ kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080
```


To switch between google cloud and minikube:

```bash
$ gcloud container cluster get-credentials mycluster # Container Engine context
$ kubectl config use-context minikube # Minikube context
```

## Build geonode django container.

Docker-compose handles building of the frontend containers. But to push to kubernetes or to google clouds private repository, we need to build the container with specific tags. From the root directory.

```bash
$ docker build -t njcoast/cyberspatial ./dockerfiles/django/production/
```

Once you’ve replaced the image attribute with the Docker image you pushed, let’s create the Service and Replication Controller:
```bash
$ kubectl create -f kubernetes_configs/frontend.yaml
```


And you can check the status with:

```bash
$ kubectl get services
NAME CLUSTER_IP EXTERNAL_IP PORT(S) SELECTOR AGE
frontend 10.67.240.157 104.197.69.33 80/TCP name=frontend
kubernetes 10.67.240.1 <none> 443/TCP <none>
```


## Kubernetes debugging tips.

From Bill Prin's medium article.
The command kubectl logs will show the standard output/error of the container, which is usually the first place to look for problems.

```bash
$ kubectl logs <pod name>
```

You can exec a shell with kubectl using:
```bash
$ kubectl exec <frontend-pod> -- cat /etc/secrets/djangouserpw
```

If you want to see all of the Kubernetes meta-information about a resource, try `kubectl describe`:

```bash
$ kubectl describe pod <pod_name>
$ kubectl describe service <service_name>
$ kubectl describe rc <rc_name>
```

When you run into problems, it sometime helps to rebuild the resource:

```bash
$ kubectl delete rc frontend
$ kubectl create -f kubernetes_configs/frontend.yaml
```

## Kubernetes, google cloud, and postgresql
You will need a google persistent disk to attach to the postgresql storage.

To create a disk in google storage:
```bash
$ gcloud compute disks create pg-data --size 500GB
```

The next step is to create a secrets file for postgres password. From the blog post:
"First, we’re going to create a Kubernetes Secret to contain our database passwords. Creating a Secret lets us store things like password or SSL certs in our clusters without adding it to the image itself or storing it an unsafe storage. You might wonder why we are using secrets for Postgres not for Redis. In both cases, we don’t play to expose either as an external Service, which means we don’t need secrets, but in both cases it’s still probably safer to use secrets to protect your data. You use your best judgement, but in this case I thought it would be helpful to demonstrate how to use Kubernetes secrets to store passwords. The secrets will be saved in base64 format, so you have to encode it. Note: base64 is a compressed format, but it’s not encryption and the password in base64 format is as insecure as plaintext. On OS X or Linux, you can use the command line:"

```bash
$ echo mysecretpassword | base64
```

Open up `kubernetes_configs/db_password.yaml` and replace <your-base64-encoded-pw-here> with the output of the above base64 command.

```yaml
ApiVersion: v1
kind: Secret
metadata:
  name: db-passwords
data:
  djangouserpw: bXlzZWNyZXRwYXNzd29yZAo=
```

Then, add the secret to the cluster:

```bash
$ kubectl create -f kubernetes_configs/db_password.yaml
```


